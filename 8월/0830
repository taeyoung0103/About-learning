

# tf2라는 이름의 가상환경을 만드는데, python버젼이 3.8인 가상환경을 만든다
conda create -n tf2 python=3.8

# 가상환경 tf2를 실행한다.
conda activate tf2(가상환경 파일명)

# tensorflow 버젼을 확인할수있다.
pip show tensorflow

# 이후 cd 파일명 으로 이동한 후 jupyter notebook을 실행하면 된다.

------------------------

Keras를 사용한 개발 과정
입력 텐서(X)와 출력 텐서(y)로 이뤄진 훈련 데이터를 정의

입력과 출력을 연결하는 Layer(층)으로 이뤄진 네트워크(모델)을 정의

Sequential 방식: 순서대로 쌓아올린 네트워크로 이뤄진 모델을 생성하는 방식
Functional API 방식: 다양한 구조의 네트워크로 이뤄진 모델을 생성하는 방식
Subclass 방식: 네트워크를 정의하는 클래스를 구현.
모델 컴파일

모델이 Train(학습)할때 사용할 손실함수(Loss Function), 최적화기법(Optimizer), 학습과정을 모니터링할 평가지표(Metrics)를 설정
Training(훈련)

모델의 fit() 메소드에 훈련데이터(X,y)를 넣어 Train

------------------------

MNIST 이미지 분류

MNIST(Modified National Institute of Standards and Technology) database
흑백 손글씨 숫자 0-9까지 10개의 범주로 구분해놓은 데이터셋
하나의 이미지는 28 * 28 pixel 의 크기
6만개의 Train 이미지와 1만개의 Test 이미지로 구성됨.

import tensorflow as tf
from tensorflow import keras
# from tensorflow import keras를 함으로써  예) tf.keras.__version__ 을 keras.__version와 같이 줄여서 할수 있다.

----------------------

# MNIST Dataset 을 Loading - keras의 toy dataset으로 제공
# (train data) 와 (test data)
(train_image, train_label), (test_image, test_label) = keras.datasets.mnist.load_data()

-----------------------------

print(train_image.shape, train_label.shape)
# train_image: (이미지개수, height, width)
# train_label: (정답개수, ) 
-> (60000, 28, 28) (60000,)

print(test_image.shape, test_label.shape)
-> (10000, 28, 28) (10000,)

---------------

# 10개 table 이미지를 확인
import matplotlib.pyplot as plt
plt.figure(figsize=(15,5))
for idx in range(10):
    plt.subplot(1,10,idx+1)
    plt.imshow(train_image[idx], cmap='gray')
    plt.title(f"Label: {train_label[idx]}", fontsize=20)
    plt.axis('off')
    
plt.tight_layout()
plt.show()
-> train set에서 1부터 10까지 10개의 라벨이 쭉 나온다.


# 10개 test 이미지를 확인
import matplotlib.pyplot as plt
plt.figure(figsize=(15,5))
for idx in range(10):
    plt.subplot(1,10,idx+1)
    plt.imshow(test_image[idx], cmap='gray')
    plt.title(f"Label: {test_label[idx]}", fontsize=20)
    plt.axis('off')
    
plt.tight_layout()
plt.show()


-------------------------------

신경망 구현

network : 전체 모델 구조 만들기

# 모델(네트워크) 만들기
model = keras.Sequential()

# 모델에 Layer들을 추가
# 1
model.add(keras.layers.InputLayer((28,28)))   # 첫번째 Layer는 Input Layer(입력데이터의 shape가 (28,28)인 2차원을 만들거라는 말)
# 2
model.add(keras.layers.Flatten())   # 28 X 28 입력데이터를 1차원 배열(784,) 로 변환
# 3
# n차원 input을 1차원 output으로 바꾸는 함수가 faltten이다.
model.add(keras.layers.Dense(units=128, activation='relu'))   # relu
# 4
model.add(keras.layers.Dense(units=64, activation='relu'))   # 64개의 유닛생성

# 5
model.add(keras.layers.Dense(units=10, activation='softmax'))   # 현재 진행중인 label개수가 10개이므로 최종은 unit 10으로 해야한다.

# dense = 추론을 도와주는 역할
# inputlayer는 입구를 표현하는 것이다.
# layer = 일하는애들을 모아놓은 그룹이라고 생각하면 편하다. unit(출력 결과 사이지 개수) 수를 점점 줄여준다.


----------------------------------------

model.summary()

# param은 학습할 대상 개수이다.

Model: "sequential_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_3 (Flatten)          (None, 784)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 128)               100480    
_________________________________________________________________
dense_4 (Dense)              (None, 64)                8256      
_________________________________________________________________
dense_5 (Dense)              (None, 10)                650       
=================================================================
Total params: 109,386
Trainable params: 109,386
Non-trainable params: 0
_________________________________________________________________

-------------------------------------


컴파일 단계
    구축된 모델에 학습을 위한 추가 설정
    손실함수
    Optimizer(최적화 함수)
    평가지표
    
model.compile(optimizer="adam",                 # 최적화함수-모델의 파라미터(weight)들을 업데이트
             loss="categorical_crossentropy",   # Loss(손실,오차)를 계산하는 함수설정. categorical_crossentropy = 다중분류의 손실함수
             metrics=['accuracy']
             )

# 설정하는것

------------------------------------------

데이터 준비
    X (Input Data Image)
        0 ~ 1 사이의 값으로 정규화 시킨다.
    y (Output Data)
        one hot encoding 처리
            Label이 다중분류일 경우 One Hot Encoding 한다.
        tensorflow.keras 의 to_categorical() 함수 이용


np.min(train_image), np.max(train_image), train_image.dtype
-> (0, 255, dtype('uint8'))

X_train = (train_image/255.0).astype(np.float32)
-> X학습 설정, 이미지값은 0부터 255인데, 0~1사이의 값으로 만들어야하니까 255.0으로 나눈다.

X_test = (test_image/255.0).astype(np.float32)
-> X시험 설정

np.min(X_train), np.max(X_train), X_train.dtype
-> (0.0, 1.0, dtype('float32'))    - 역시 최소값은 0, 최대값은 1, 타입은 float32로 변했음을 확인할 수 있다.

train_label, np.unique(train_label)
-> (array([5, 0, 4, ..., 5, 6, 8], dtype=uint8),        - train_label 값들이 쭉나온다.
 array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8))       - np.unique 즉 고유값들만 나오게하는 함수를 사용했으므로, 고유값들인 0부터 9까지의 수가 나온다.
 
 train_label[:5]
 -> array([5, 0, 4, 1, 9], dtype=uint8)              -  train_label의 처음 5개가 (5, 0, 4, 1, 9) 이고, 타입은 unit8이다.
 
 
 
 # Label onehot encoding
y_train = keras.utils.to_categorical(train_label)
y_test = keras.utils.to_categorical(test_label)
-> y값의 train과 test, 학습과 시험 설정


print(y_train.shape, train_label.shape)
y_train[:5]

# 0  1  2  3  4  5  6  7  8  9  의 순서에서 첫번째 줄 5에 위치한 것을 1로, 2번째 줄에 0에 위치한 것을 1로, 
# 3번쨰 줄에 4에 위치한 것을 1로, 4번째 줄에 1에 위치한 것을 1로, 5번째 줄에 9에 위치한 것을 1로 바꾼다.

-> (60000, 10) (60000,)     
-> array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)
       
- 처음 5개가 5,0,4,1,9 였는데, 원핫인코딩으로 바꾼것을 봐도 첫번째줄은 5에 1, 두번째줄은 0에 1, 이렇게 나타남을 확인할 수 있다.


--------------------------------

학습


# 1 epoch: train dataset 전체를 한번 학습하는 것
# 1 step : 파라미터(weight)를 한번 update
hist = model.fit(X_train, y_train, # train dataset - x, y
         epochs=10,       # epoch 수 - train dataset을 몇번 학습할지 지정하는것. 여기서는 10번한다는 말 
         batch_size=100,   # 파라미터(weight - 모델의 학습대상)을 몇개의 데이터마다 업데이트할지 설정. 여기서는 100개씩 나눠서 업데이트
         validation_split=0.3   # train set의 30%는 검증 데이터셋으로 사용하겠다는 의미이다.
         )

# 학습데이터 : 60000개, batch_size: 100
# 1 epoch당 600 step
# loss = 손실, 오차값 
# 42000개의 train set과 18000개의 valid set이 있는데, val이 붙은것이 valid set의 결과이다.
# train set의 결과값들은 실행할수록 loss는 줄어들고, accuracy값은 증가하는반면
# val는 올라갔다 내려갔다하는것을 볼수있는데, epoch수가 증가할수록 과적합이 일어나며, 어느 지점에 가면 올라갔다 내려갔다하는것을 확인할 수 있다.












